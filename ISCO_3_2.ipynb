{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3, Question 1, ISCO630E \n",
    "### Submitted by Bhanu Bhandari (IEC2016027)\n",
    "#### Logistic Regression to predict the Admission or Rejection of a student into a program based on marks in two tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was in a docx file so it was first converted into a csv file in order to extract it via pandas. Before looking at which kind of hypothesis we can use to separate the data and obtain a decision boundary, we first explore and visualise the data we have at hand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = pd.read_csv(\"/Users/bhanubhandari/Desktop/GCN_3/data_admission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parameter1</th>\n",
       "      <th>parameter2</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.623660</td>\n",
       "      <td>78.024693</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.286711</td>\n",
       "      <td>43.894998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35.847409</td>\n",
       "      <td>72.902198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60.182599</td>\n",
       "      <td>86.308552</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79.032736</td>\n",
       "      <td>75.344376</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>45.083277</td>\n",
       "      <td>56.316372</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>61.106665</td>\n",
       "      <td>96.511426</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>75.024746</td>\n",
       "      <td>46.554014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>76.098787</td>\n",
       "      <td>87.420570</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>84.432820</td>\n",
       "      <td>43.533393</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>95.861555</td>\n",
       "      <td>38.225278</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>75.013658</td>\n",
       "      <td>30.603263</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>82.307053</td>\n",
       "      <td>76.481963</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>69.364589</td>\n",
       "      <td>97.718692</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>39.538339</td>\n",
       "      <td>76.036811</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>53.971052</td>\n",
       "      <td>89.207350</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>69.070144</td>\n",
       "      <td>52.740470</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>67.946855</td>\n",
       "      <td>46.678574</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>70.661510</td>\n",
       "      <td>92.927138</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>76.978784</td>\n",
       "      <td>47.575964</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>67.372028</td>\n",
       "      <td>42.838438</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>89.676776</td>\n",
       "      <td>65.799366</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>50.534788</td>\n",
       "      <td>48.855812</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>34.212061</td>\n",
       "      <td>44.209529</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>77.924091</td>\n",
       "      <td>68.972360</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>62.271014</td>\n",
       "      <td>69.954458</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>80.190181</td>\n",
       "      <td>44.821629</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>93.114389</td>\n",
       "      <td>38.800670</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>61.830206</td>\n",
       "      <td>50.256108</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>38.785804</td>\n",
       "      <td>64.995681</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>32.722833</td>\n",
       "      <td>43.307173</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>64.039320</td>\n",
       "      <td>78.031688</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>72.346494</td>\n",
       "      <td>96.227593</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>60.457886</td>\n",
       "      <td>73.094998</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>58.840956</td>\n",
       "      <td>75.858448</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>99.827858</td>\n",
       "      <td>72.369252</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>47.264269</td>\n",
       "      <td>88.475865</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>50.458160</td>\n",
       "      <td>75.809860</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>60.455556</td>\n",
       "      <td>42.508409</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>82.226662</td>\n",
       "      <td>42.719879</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>88.913896</td>\n",
       "      <td>69.803789</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>94.834507</td>\n",
       "      <td>45.694307</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>67.319257</td>\n",
       "      <td>66.589353</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>57.238706</td>\n",
       "      <td>59.514282</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>80.366756</td>\n",
       "      <td>90.960148</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>68.468522</td>\n",
       "      <td>85.594307</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>42.075455</td>\n",
       "      <td>78.844786</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>75.477702</td>\n",
       "      <td>90.424539</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>78.635424</td>\n",
       "      <td>96.647427</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>52.348004</td>\n",
       "      <td>60.769505</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>94.094331</td>\n",
       "      <td>77.159105</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>90.448551</td>\n",
       "      <td>87.508792</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>55.482161</td>\n",
       "      <td>35.570703</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>74.492692</td>\n",
       "      <td>84.845137</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>89.845807</td>\n",
       "      <td>45.358284</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>83.489163</td>\n",
       "      <td>48.380286</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>42.261701</td>\n",
       "      <td>87.103851</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>99.315009</td>\n",
       "      <td>68.775409</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>55.340018</td>\n",
       "      <td>64.931938</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>74.775893</td>\n",
       "      <td>89.529813</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    parameter1  parameter2  result\n",
       "0    34.623660   78.024693       0\n",
       "1    30.286711   43.894998       0\n",
       "2    35.847409   72.902198       0\n",
       "3    60.182599   86.308552       1\n",
       "4    79.032736   75.344376       1\n",
       "5    45.083277   56.316372       0\n",
       "6    61.106665   96.511426       1\n",
       "7    75.024746   46.554014       1\n",
       "8    76.098787   87.420570       1\n",
       "9    84.432820   43.533393       1\n",
       "10   95.861555   38.225278       0\n",
       "11   75.013658   30.603263       0\n",
       "12   82.307053   76.481963       1\n",
       "13   69.364589   97.718692       1\n",
       "14   39.538339   76.036811       0\n",
       "15   53.971052   89.207350       1\n",
       "16   69.070144   52.740470       1\n",
       "17   67.946855   46.678574       0\n",
       "18   70.661510   92.927138       1\n",
       "19   76.978784   47.575964       1\n",
       "20   67.372028   42.838438       0\n",
       "21   89.676776   65.799366       1\n",
       "22   50.534788   48.855812       0\n",
       "23   34.212061   44.209529       0\n",
       "24   77.924091   68.972360       1\n",
       "25   62.271014   69.954458       1\n",
       "26   80.190181   44.821629       1\n",
       "27   93.114389   38.800670       0\n",
       "28   61.830206   50.256108       0\n",
       "29   38.785804   64.995681       0\n",
       "..         ...         ...     ...\n",
       "70   32.722833   43.307173       0\n",
       "71   64.039320   78.031688       1\n",
       "72   72.346494   96.227593       1\n",
       "73   60.457886   73.094998       1\n",
       "74   58.840956   75.858448       1\n",
       "75   99.827858   72.369252       1\n",
       "76   47.264269   88.475865       1\n",
       "77   50.458160   75.809860       1\n",
       "78   60.455556   42.508409       0\n",
       "79   82.226662   42.719879       0\n",
       "80   88.913896   69.803789       1\n",
       "81   94.834507   45.694307       1\n",
       "82   67.319257   66.589353       1\n",
       "83   57.238706   59.514282       1\n",
       "84   80.366756   90.960148       1\n",
       "85   68.468522   85.594307       1\n",
       "86   42.075455   78.844786       0\n",
       "87   75.477702   90.424539       1\n",
       "88   78.635424   96.647427       1\n",
       "89   52.348004   60.769505       0\n",
       "90   94.094331   77.159105       1\n",
       "91   90.448551   87.508792       1\n",
       "92   55.482161   35.570703       0\n",
       "93   74.492692   84.845137       1\n",
       "94   89.845807   45.358284       1\n",
       "95   83.489163   48.380286       1\n",
       "96   42.261701   87.103851       1\n",
       "97   99.315009   68.775409       1\n",
       "98   55.340018   64.931938       1\n",
       "99   74.775893   89.529813       1\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = input_data[[\"parameter1\",\"parameter2\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[34.62365962, 78.02469282],\n",
       "       [30.28671077, 43.89499752],\n",
       "       [35.84740877, 72.90219803],\n",
       "       [60.18259939, 86.3085521 ],\n",
       "       [79.03273605, 75.34437644],\n",
       "       [45.08327748, 56.31637178],\n",
       "       [61.10666454, 96.51142588],\n",
       "       [75.02474557, 46.55401354],\n",
       "       [76.0987867 , 87.42056972],\n",
       "       [84.43281996, 43.53339331],\n",
       "       [95.86155507, 38.22527806],\n",
       "       [75.01365839, 30.60326323],\n",
       "       [82.30705337, 76.4819633 ],\n",
       "       [69.36458876, 97.71869196],\n",
       "       [39.53833914, 76.03681085],\n",
       "       [53.97105215, 89.20735014],\n",
       "       [69.07014406, 52.74046973],\n",
       "       [67.94685548, 46.67857411],\n",
       "       [70.66150955, 92.92713789],\n",
       "       [76.97878373, 47.57596365],\n",
       "       [67.37202755, 42.83843832],\n",
       "       [89.67677575, 65.79936593],\n",
       "       [50.53478829, 48.85581153],\n",
       "       [34.21206098, 44.2095286 ],\n",
       "       [77.92409145, 68.97235999],\n",
       "       [62.27101367, 69.95445795],\n",
       "       [80.19018075, 44.82162893],\n",
       "       [93.1143888 , 38.80067034],\n",
       "       [61.83020602, 50.25610789],\n",
       "       [38.7858038 , 64.99568096],\n",
       "       [61.37928945, 72.80788731],\n",
       "       [85.40451939, 57.05198398],\n",
       "       [52.10797973, 63.12762377],\n",
       "       [52.04540477, 69.43286012],\n",
       "       [40.23689374, 71.16774802],\n",
       "       [54.63510555, 52.21388588],\n",
       "       [33.91550011, 98.86943574],\n",
       "       [64.17698887, 80.90806059],\n",
       "       [74.78925296, 41.57341523],\n",
       "       [34.18364003, 75.23772034],\n",
       "       [83.90239366, 56.30804622],\n",
       "       [51.54772027, 46.85629026],\n",
       "       [94.44336777, 65.56892161],\n",
       "       [82.36875376, 40.61825516],\n",
       "       [51.04775177, 45.82270146],\n",
       "       [62.22267576, 52.06099195],\n",
       "       [77.19303493, 70.4582    ],\n",
       "       [97.77159928, 86.72782233],\n",
       "       [62.0730638 , 96.76882412],\n",
       "       [91.5649745 , 88.69629255],\n",
       "       [79.94481794, 74.16311935],\n",
       "       [99.27252693, 60.999031  ],\n",
       "       [90.54671411, 43.39060181],\n",
       "       [34.52451385, 60.39634246],\n",
       "       [50.28649612, 49.80453881],\n",
       "       [49.58667722, 59.80895099],\n",
       "       [97.64563396, 68.86157272],\n",
       "       [32.57720017, 95.59854761],\n",
       "       [74.24869137, 69.82457123],\n",
       "       [71.79646206, 78.45356225],\n",
       "       [75.39561147, 85.75993667],\n",
       "       [35.28611282, 47.02051395],\n",
       "       [56.2538175 , 39.26147251],\n",
       "       [30.05882245, 49.59297387],\n",
       "       [44.66826172, 66.45008615],\n",
       "       [66.56089447, 41.09209808],\n",
       "       [40.45755098, 97.53518549],\n",
       "       [49.07256322, 51.88321182],\n",
       "       [80.27957401, 92.11606081],\n",
       "       [66.74671857, 60.99139403],\n",
       "       [32.72283304, 43.30717306],\n",
       "       [64.03932042, 78.03168802],\n",
       "       [72.34649423, 96.22759297],\n",
       "       [60.45788574, 73.0949981 ],\n",
       "       [58.84095622, 75.85844831],\n",
       "       [99.8278578 , 72.36925193],\n",
       "       [47.26426911, 88.475865  ],\n",
       "       [50.4581598 , 75.80985953],\n",
       "       [60.45555629, 42.50840944],\n",
       "       [82.22666158, 42.71987854],\n",
       "       [88.91389642, 69.8037889 ],\n",
       "       [94.83450672, 45.6943068 ],\n",
       "       [67.31925747, 66.58935318],\n",
       "       [57.23870632, 59.51428198],\n",
       "       [80.366756  , 90.9601479 ],\n",
       "       [68.46852179, 85.5943071 ],\n",
       "       [42.07545454, 78.844786  ],\n",
       "       [75.47770201, 90.424539  ],\n",
       "       [78.63542435, 96.64742717],\n",
       "       [52.34800399, 60.76950526],\n",
       "       [94.09433113, 77.15910509],\n",
       "       [90.44855097, 87.50879176],\n",
       "       [55.48216114, 35.57070347],\n",
       "       [74.49269242, 84.84513685],\n",
       "       [89.84580671, 45.35828361],\n",
       "       [83.48916274, 48.3802858 ],\n",
       "       [42.26170081, 87.10385094],\n",
       "       [99.31500881, 68.77540947],\n",
       "       [55.34001756, 64.93193801],\n",
       "       [74.775893  , 89.5298129 ]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = (input_data[\"result\"]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "       0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For clarity of the data and the next steps for our predictions, we now visualise the data at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2) (100,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(X.shape, Y.shape)\n",
    "for i in range(X.shape[0]):\n",
    "    if Y[i]==1:\n",
    "        plt.plot(X[i,0],X[i,1],'rx')\n",
    "    else:\n",
    "        plt.plot(X[i,0],X[i,1],'gx')\n",
    "plt.xlabel('Parameter 1')\n",
    "plt.ylabel('Parameter 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that the boundary between selection and rejection is fairly linear in nature, that is we can use a linear hypothesis to predict the values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we go forward with the hypothesis \n",
    "$$ y = w_0 + w_1x_1 + w_2x_2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68, 2) (32, 2) (68,) (32,)\n"
     ]
    }
   ],
   "source": [
    "random_val_msk = np.random.rand(len(X)) < 0.7\n",
    "X_train = X[random_val_msk]\n",
    "X_test = X[~random_val_msk]\n",
    "Y_train = Y[random_val_msk]\n",
    "Y_test = Y[~random_val_msk] \n",
    "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the train-test split, we shall proceed to train the data with and without normalisation in order to see the results of both. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68, 3)\n"
     ]
    }
   ],
   "source": [
    "# Adding row for bias term w0 in the hypothesis to the entire dataset \n",
    "X_train = np.concatenate( (np.ones((X_train.shape[0],1)),X_train) , axis=1)\n",
    "X_test = np.concatenate( (np.ones((X_test.shape[0],1)),X_test) , axis=1)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.98919317 0.11733969 0.39062603]]\n",
      "(68,)\n"
     ]
    }
   ],
   "source": [
    "m,n = X_train.shape # n is the shape of original_X.shape[0]+1\n",
    "weights = np.random.rand(1,n)\n",
    "print(weights)\n",
    "h = sigmoid(X_train@weights.T)\n",
    "h = h.reshape(-1)\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X,Y,theta,alpha,lambda1,iterations):\n",
    "    i=0\n",
    "    while(i<iterations):\n",
    "        sigmoid_output = sigmoid(X@theta.T).reshape(-1)\n",
    "        theta = theta - (alpha/m)*(X.T@(sigmoid_output-Y)) - (lambda1/m)*np.sum(theta[:,1:])\n",
    "        i+=1\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate value = 0.0001, Accuracy % = 54.83870967741935\n",
      "Learning rate value = 0.0005, Accuracy % = 70.96774193548387\n",
      "Learning rate value = 0.001, Accuracy % = 80.64516129032258\n",
      "Learning rate value = 0.05, Accuracy % = 83.87096774193549\n",
      "Learning rate value = 0.01, Accuracy % = 83.87096774193549\n",
      "Learning rate value = 0.1, Accuracy % = 83.87096774193549\n",
      "Learning rate value = 0.15, Accuracy % = 83.87096774193549\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.0001, 0.0005, 0.0010, 0.050, 0.0100, 0.100, 0.150] \n",
    "epochs = 100000\n",
    "\n",
    "for alpha in alphas:\n",
    "    theta_x = gradient_descent(X_train,Y_train,weights,alpha,0,epochs) \n",
    "    h = sigmoid(X_test@theta_x.T)\n",
    "    h = ((h>=0.5)*1).reshape(-1)\n",
    "    accuracy = (np.sum(h==Y_test)/Y_test.shape[0])*100\n",
    "    print(\"Learning rate value = \" + str(alpha) + \", Accuracy % = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda = 0\n",
      "Learning rate value: 0.0001, Accuracy % = 50.0\n",
      "Learning rate value: 0.0005, Accuracy % = 59.375\n",
      "Learning rate value: 0.001, Accuracy % = 75.0\n",
      "Learning rate value: 0.05, Accuracy % = 81.25\n",
      "Learning rate value: 0.01, Accuracy % = 59.375\n",
      "Learning rate value: 0.1, Accuracy % = 84.375\n",
      "Learning rate value: 0.15, Accuracy % = 81.25\n",
      "End of iteration \n",
      "\n",
      "Lambda = 0.001\n",
      "Learning rate value: 0.0001, Accuracy % = 50.0\n",
      "Learning rate value: 0.0005, Accuracy % = 59.375\n",
      "Learning rate value: 0.001, Accuracy % = 75.0\n",
      "Learning rate value: 0.05, Accuracy % = 84.375\n",
      "Learning rate value: 0.01, Accuracy % = 87.5\n",
      "Learning rate value: 0.1, Accuracy % = 87.5\n",
      "Learning rate value: 0.15, Accuracy % = 78.125\n",
      "End of iteration \n",
      "\n",
      "Lambda = 0.005\n",
      "Learning rate value: 0.0001, Accuracy % = 50.0\n",
      "Learning rate value: 0.0005, Accuracy % = 59.375\n",
      "Learning rate value: 0.001, Accuracy % = 75.0\n",
      "Learning rate value: 0.05, Accuracy % = 78.125\n",
      "Learning rate value: 0.01, Accuracy % = 81.25\n",
      "Learning rate value: 0.1, Accuracy % = 81.25\n",
      "Learning rate value: 0.15, Accuracy % = 59.375\n",
      "End of iteration \n",
      "\n",
      "Lambda = 0.01\n",
      "Learning rate value: 0.0001, Accuracy % = 50.0\n",
      "Learning rate value: 0.0005, Accuracy % = 59.375\n",
      "Learning rate value: 0.001, Accuracy % = 78.125\n",
      "Learning rate value: 0.05, Accuracy % = 81.25\n",
      "Learning rate value: 0.01, Accuracy % = 59.375\n",
      "Learning rate value: 0.1, Accuracy % = 78.125\n",
      "Learning rate value: 0.15, Accuracy % = 59.375\n",
      "End of iteration \n",
      "\n",
      "Lambda = 0.05\n",
      "Learning rate value: 0.0001, Accuracy % = 59.375\n",
      "Learning rate value: 0.0005, Accuracy % = 78.125\n",
      "Learning rate value: 0.001, Accuracy % = 84.375\n",
      "Learning rate value: 0.05, Accuracy % = 84.375\n",
      "Learning rate value: 0.01, Accuracy % = 84.375\n",
      "Learning rate value: 0.1, Accuracy % = 84.375\n",
      "Learning rate value: 0.15, Accuracy % = 84.375\n",
      "End of iteration \n",
      "\n",
      "Lambda = 0.1\n",
      "Learning rate value: 0.0001, Accuracy % = 84.375\n",
      "Learning rate value: 0.0005, Accuracy % = 87.5\n",
      "Learning rate value: 0.001, Accuracy % = 84.375\n",
      "Learning rate value: 0.05, Accuracy % = 84.375\n",
      "Learning rate value: 0.01, Accuracy % = 84.375\n",
      "Learning rate value: 0.1, Accuracy % = 84.375\n",
      "Learning rate value: 0.15, Accuracy % = 84.375\n",
      "End of iteration \n",
      "\n",
      "Lambda = 0.5\n",
      "Learning rate value: 0.0001, Accuracy % = 50.0\n",
      "Learning rate value: 0.0005, Accuracy % = 50.0\n",
      "Learning rate value: 0.001, Accuracy % = 50.0\n",
      "Learning rate value: 0.05, Accuracy % = 50.0\n",
      "Learning rate value: 0.01, Accuracy % = 50.0\n",
      "Learning rate value: 0.1, Accuracy % = 50.0\n",
      "Learning rate value: 0.15, Accuracy % = 50.0\n",
      "End of iteration \n",
      "\n",
      "Lambda = 1\n",
      "Learning rate value: 0.0001, Accuracy % = 50.0\n",
      "Learning rate value: 0.0005, Accuracy % = 50.0\n",
      "Learning rate value: 0.001, Accuracy % = 50.0\n",
      "Learning rate value: 0.05, Accuracy % = 50.0\n",
      "Learning rate value: 0.01, Accuracy % = 50.0\n",
      "Learning rate value: 0.1, Accuracy % = 50.0\n",
      "Learning rate value: 0.15, Accuracy % = 50.0\n",
      "End of iteration \n",
      "\n",
      "Lambda = 5\n",
      "Learning rate value: 0.0001, Accuracy % = 50.0\n",
      "Learning rate value: 0.0005, Accuracy % = 50.0\n",
      "Learning rate value: 0.001, Accuracy % = 50.0\n",
      "Learning rate value: 0.05, Accuracy % = 50.0\n",
      "Learning rate value: 0.01, Accuracy % = 50.0\n",
      "Learning rate value: 0.1, Accuracy % = 50.0\n",
      "Learning rate value: 0.15, Accuracy % = 50.0\n",
      "End of iteration \n",
      "\n",
      "Lambda = 10\n",
      "Learning rate value: 0.0001, Accuracy % = 50.0\n",
      "Learning rate value: 0.0005, Accuracy % = 50.0\n",
      "Learning rate value: 0.001, Accuracy % = 50.0\n",
      "Learning rate value: 0.05, Accuracy % = 50.0\n",
      "Learning rate value: 0.01, Accuracy % = 50.0\n",
      "Learning rate value: 0.1, Accuracy % = 50.0\n",
      "Learning rate value: 0.15, Accuracy % = 50.0\n",
      "End of iteration \n",
      "\n",
      "Lambda = 20\n",
      "Learning rate value: 0.0001, Accuracy % = 50.0\n",
      "Learning rate value: 0.0005, Accuracy % = 50.0\n",
      "Learning rate value: 0.001, Accuracy % = 50.0\n",
      "Learning rate value: 0.05, Accuracy % = 50.0\n",
      "Learning rate value: 0.01, Accuracy % = 50.0\n",
      "Learning rate value: 0.1, Accuracy % = 50.0\n",
      "Learning rate value: 0.15, Accuracy % = 50.0\n",
      "End of iteration \n",
      "\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.0001, 0.0005, 0.0010, 0.050, 0.0100, 0.100, 0.150] \n",
    "lambdas = [0,0.001,0.005,0.01,0.05,0.1,0.5,1,5,10,20]\n",
    "epochs = 100000\n",
    "\n",
    "for lambda1 in lambdas:\n",
    "    print(\"Lambda = \" + str(lambda1))\n",
    "    for alpha in alphas:\n",
    "        theta_x = gradient_descent(X_train,Y_train,weights,alpha,lambda1,epochs)\n",
    "        hypothesis = sigmoid(X_test@theta_x.T)\n",
    "        hypothesis = (hypothesis>=0.5)*1\n",
    "        hypothesis = hypothesis.reshape(-1)\n",
    "        truth = hypothesis==Y_test\n",
    "        acc = (np.sum(truth)/Y_test.shape[0])*100\n",
    "        print(\"Learning rate value: \" + str(alpha) + \", Accuracy % = \" + str(acc))\n",
    "    print(\"End of iteration \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we can observe that without normalisation as well, the linear hypothesis for logistic regression performs very well, with a best accuracy of around 87.5%. However, normalisation with or without regularisation can indeed help improve our accuracy by putting the values close together in a range between 0 and 1. \n",
    "\n",
    "Now, we perform gradient descent with normalisation and observe the differences. \n",
    "\n",
    "## GRADIENT DESCENT WITH NORMALISED VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00326632 0.19470455]\n",
      " [0.08296784 0.61961779]\n",
      " [0.43176427 0.81600135]\n",
      " [0.7019434  0.65539214]\n",
      " [0.2153456  0.37665959]\n",
      " [0.44500891 0.96545859]\n",
      " [0.64449684 0.23365526]\n",
      " [0.65989108 0.83229079]\n",
      " [0.77934283 0.18940757]\n",
      " [0.94315096 0.11165142]\n",
      " [0.64433793 0.        ]\n",
      " [0.74887421 0.67205614]\n",
      " [0.56336978 0.98314328]\n",
      " [0.34273413 0.85846452]\n",
      " [0.55914951 0.32427783]\n",
      " [0.5430494  0.23547989]\n",
      " [0.67250409 0.24862534]\n",
      " [0.53481039 0.17922749]\n",
      " [0.29348214 0.26737325]\n",
      " [0.05952839 0.19931197]\n",
      " [0.68605319 0.56205138]\n",
      " [0.71853306 0.20827835]\n",
      " [0.45537943 0.28788555]\n",
      " [0.12508388 0.50379883]\n",
      " [0.44891644 0.61823627]\n",
      " [0.7932702  0.38743524]\n",
      " [0.3160307  0.47643451]\n",
      " [0.05527778 1.        ]\n",
      " [0.64112153 0.16069675]\n",
      " [0.05912103 0.65382979]\n",
      " [0.77174023 0.37653763]\n",
      " [0.3080005  0.23808317]\n",
      " [0.74975856 0.14670505]\n",
      " [0.30083445 0.2229426 ]\n",
      " [0.4610047  0.31432447]\n",
      " [0.67557495 0.58381678]\n",
      " [0.97052763 0.82214305]\n",
      " [0.88156804 0.85097827]\n",
      " [0.71501627 0.63808845]\n",
      " [0.99204044 0.44525373]\n",
      " [0.86697331 0.18731589]\n",
      " [0.06400678 0.43642522]\n",
      " [0.28992337 0.28127072]\n",
      " [0.27989286 0.42782079]\n",
      " [0.96872217 0.56042851]\n",
      " [0.63337365 0.57453504]\n",
      " [0.59822584 0.70093719]\n",
      " [0.07492278 0.24048881]\n",
      " [0.37545302 0.12683016]\n",
      " [0.         0.2781716 ]\n",
      " [0.20939718 0.52510375]\n",
      " [0.52318442 0.15364615]\n",
      " [0.14904504 0.98045518]\n",
      " [0.71981433 0.90107289]\n",
      " [0.52584783 0.44514186]\n",
      " [0.48704268 0.69475735]\n",
      " [0.43570996 0.62244203]\n",
      " [1.         0.6118109 ]\n",
      " [0.29238382 0.66221079]\n",
      " [0.43567657 0.17439305]\n",
      " [0.74772195 0.17749077]\n",
      " [0.84357013 0.57423061]\n",
      " [0.53405404 0.52714381]\n",
      " [0.38956944 0.42350432]\n",
      " [0.72106391 0.88414045]\n",
      " [0.17223446 0.70666805]\n",
      " [0.31947097 0.44189151]\n",
      " [0.91782133 0.68197528]\n",
      " [0.86556634 0.83358311]\n",
      " [0.8569272  0.21613956]\n",
      " [0.17490393 0.82765132]\n",
      " [0.99264933 0.55916635]\n",
      " [0.64093004 0.86318813]] [[0.03287099 0.69371394]\n",
      " [0.1118124  0.66123121]\n",
      " [0.61172433 0.93722542]\n",
      " [0.91715461 0.49394756]\n",
      " [0.47695307 0.56184331]\n",
      " [0.97237083 0.05277886]\n",
      " [0.31270554 0.5533202 ]\n",
      " [0.12303285 0.58166892]\n",
      " [0.35430228 0.27195578]\n",
      " [0.50756755 0.74082924]\n",
      " [0.99371738 0.49018202]\n",
      " [0.47377353 1.        ]\n",
      " [0.         0.98087725]\n",
      " [0.68776524 0.82011069]\n",
      " [0.26495465 0.26655244]\n",
      " [0.00233921 0.12641678]\n",
      " [0.63878918 0.99115608]\n",
      " [0.42185821 0.6583167 ]\n",
      " [0.23590916 0.86448997]\n",
      " [1.         0.16542343]\n",
      " [0.57649975 0.81740425]\n",
      " [0.68908381 0.89633203]\n",
      " [0.73980432 0.99801633]\n",
      " [0.367908   0.        ]\n",
      " [0.67326222 0.80516253]\n",
      " [0.81776687 0.20931333]\n",
      " [0.36562483 0.47977347]]\n"
     ]
    }
   ],
   "source": [
    "X = input_data[[\"parameter1\",\"parameter2\"]].values\n",
    "random_val_msk = np.random.rand(len(X)) < 0.7\n",
    "X_train = X[random_val_msk]\n",
    "X_test = X[~random_val_msk]\n",
    "Y_train = Y[random_val_msk]\n",
    "Y_test = Y[~random_val_msk]\n",
    "X_train = (X_train-np.min(X_train,axis=0))/(np.max(X_train,axis=0)-np.min(X_train,axis=0))\n",
    "X_test = (X_test-np.min(X_test,axis=0))/(np.max(X_test,axis=0)-np.min(X_test,axis=0))\n",
    "print(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then add a bias term to the hypothesis we assumed earlier, as we did previously in order to satisfy the matrix dimensions for calculating the gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05971716 0.61983187 0.9697338 ]]\n",
      "(73,)\n"
     ]
    }
   ],
   "source": [
    "# Adding row for bias term w0 in the hypothesis to the entire dataset \n",
    "X_train = np.concatenate( (np.ones((X_train.shape[0],1)),X_train) , axis=1)\n",
    "X_test = np.concatenate( (np.ones((X_test.shape[0],1)),X_test) , axis=1)\n",
    "m,n = X_train.shape # n is the shape of original_X.shape[0]+1\n",
    "weights = np.random.rand(1,n)\n",
    "print(weights)\n",
    "h = sigmoid(X_train@weights.T)\n",
    "h = h.reshape(-1)\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run the gradient descent algorithm after assuming certain random values of the weights to start the analysis, and then we keep on improving on those values by using gradient descent. Firstly, we run the gradient descent algorithm using just the normalised data with variable learning rates. We later on run the same analysis with varied values of the regulariser **lambda**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WITHOUT REGULARISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate value = 0.0001, Accuracy % = 77.77777777777779\n",
      "Learning rate value = 0.0005, Accuracy % = 92.5925925925926\n",
      "Learning rate value = 0.001, Accuracy % = 85.18518518518519\n",
      "Learning rate value = 0.05, Accuracy % = 85.18518518518519\n",
      "Learning rate value = 0.01, Accuracy % = 85.18518518518519\n",
      "Learning rate value = 0.1, Accuracy % = 85.18518518518519\n",
      "Learning rate value = 0.15, Accuracy % = 85.18518518518519\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.0001, 0.0005, 0.0010, 0.050, 0.0100, 0.100, 0.150] \n",
    "epochs = 100000\n",
    "\n",
    "for alpha in alphas:\n",
    "    theta_x = gradient_descent(X_train,Y_train,weights,alpha,0,epochs) \n",
    "    hypothesis = sigmoid(X_test@theta_x.T)\n",
    "    hypothesis = ((hypothesis>=0.5)*1).reshape(-1)\n",
    "    accuracy = (np.sum(hypothesis==Y_test)/Y_test.shape[0])*100\n",
    "    print(\"Learning rate value = \" + str(alpha) + \", Accuracy % = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WITH REGULARISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda = 0\n",
      "Learning rate value: 0.0001, Accuracy % = 77.77777777777779\n",
      "Learning rate value: 0.0005, Accuracy % = 92.5925925925926\n",
      "Learning rate value: 0.001, Accuracy % = 85.18518518518519\n",
      "Learning rate value: 0.05, Accuracy % = 85.18518518518519\n",
      "Learning rate value: 0.01, Accuracy % = 85.18518518518519\n",
      "Learning rate value: 0.1, Accuracy % = 85.18518518518519\n",
      "Learning rate value: 0.15, Accuracy % = 85.18518518518519\n",
      "End of iteration \n",
      "\n",
      "Lambda = 0.001\n",
      "Learning rate value: 0.0001, Accuracy % = 66.66666666666666\n",
      "Learning rate value: 0.0005, Accuracy % = 88.88888888888889\n",
      "Learning rate value: 0.001, Accuracy % = 88.88888888888889\n",
      "Learning rate value: 0.05, Accuracy % = 85.18518518518519\n",
      "Learning rate value: 0.01, Accuracy % = 85.18518518518519\n",
      "Learning rate value: 0.1, Accuracy % = 85.18518518518519\n",
      "Learning rate value: 0.15, Accuracy % = 85.18518518518519\n",
      "End of iteration \n",
      "\n",
      "Lambda = 0.005\n",
      "Learning rate value: 0.0001, Accuracy % = 33.33333333333333\n",
      "Learning rate value: 0.0005, Accuracy % = 51.85185185185185\n",
      "Learning rate value: 0.001, Accuracy % = 74.07407407407408\n",
      "Learning rate value: 0.05, Accuracy % = 85.18518518518519\n",
      "Learning rate value: 0.01, Accuracy % = 88.88888888888889\n",
      "Learning rate value: 0.1, Accuracy % = 85.18518518518519\n",
      "Learning rate value: 0.15, Accuracy % = 85.18518518518519\n",
      "End of iteration \n",
      "\n",
      "Lambda = 0.01\n",
      "Learning rate value: 0.0001, Accuracy % = 33.33333333333333\n",
      "Learning rate value: 0.0005, Accuracy % = 33.33333333333333\n",
      "Learning rate value: 0.001, Accuracy % = 44.44444444444444\n",
      "Learning rate value: 0.05, Accuracy % = 88.88888888888889\n",
      "Learning rate value: 0.01, Accuracy % = 88.88888888888889\n",
      "Learning rate value: 0.1, Accuracy % = 85.18518518518519\n",
      "Learning rate value: 0.15, Accuracy % = 85.18518518518519\n",
      "End of iteration \n",
      "\n",
      "Lambda = 0.05\n",
      "Learning rate value: 0.0001, Accuracy % = 33.33333333333333\n",
      "Learning rate value: 0.0005, Accuracy % = 33.33333333333333\n",
      "Learning rate value: 0.001, Accuracy % = 33.33333333333333\n",
      "Learning rate value: 0.05, Accuracy % = 88.88888888888889\n",
      "Learning rate value: 0.01, Accuracy % = 74.07407407407408\n",
      "Learning rate value: 0.1, Accuracy % = 88.88888888888889\n",
      "Learning rate value: 0.15, Accuracy % = 88.88888888888889\n",
      "End of iteration \n",
      "\n",
      "Lambda = 0.1\n",
      "Learning rate value: 0.0001, Accuracy % = 33.33333333333333\n",
      "Learning rate value: 0.0005, Accuracy % = 33.33333333333333\n",
      "Learning rate value: 0.001, Accuracy % = 33.33333333333333\n",
      "Learning rate value: 0.05, Accuracy % = 77.77777777777779\n",
      "Learning rate value: 0.01, Accuracy % = 44.44444444444444\n",
      "Learning rate value: 0.1, Accuracy % = 88.88888888888889\n",
      "Learning rate value: 0.15, Accuracy % = 88.88888888888889\n",
      "End of iteration \n",
      "\n",
      "Lambda = 0.5\n",
      "Learning rate value: 0.0001, Accuracy % = 33.33333333333333\n",
      "Learning rate value: 0.0005, Accuracy % = 33.33333333333333\n",
      "Learning rate value: 0.001, Accuracy % = 29.629629629629626\n",
      "Learning rate value: 0.05, Accuracy % = 44.44444444444444\n",
      "Learning rate value: 0.01, Accuracy % = 33.33333333333333\n",
      "Learning rate value: 0.1, Accuracy % = 74.07407407407408\n",
      "Learning rate value: 0.15, Accuracy % = 77.77777777777779\n",
      "End of iteration \n",
      "\n",
      "Lambda = 1\n",
      "Learning rate value: 0.0001, Accuracy % = 33.33333333333333\n",
      "Learning rate value: 0.0005, Accuracy % = 33.33333333333333\n",
      "Learning rate value: 0.001, Accuracy % = 29.629629629629626\n",
      "Learning rate value: 0.05, Accuracy % = 40.74074074074074\n",
      "Learning rate value: 0.01, Accuracy % = 33.33333333333333\n",
      "Learning rate value: 0.1, Accuracy % = 44.44444444444444\n",
      "Learning rate value: 0.15, Accuracy % = 66.66666666666666\n",
      "End of iteration \n",
      "\n",
      "Lambda = 5\n",
      "Learning rate value: 0.0001, Accuracy % = 33.33333333333333\n",
      "Learning rate value: 0.0005, Accuracy % = 33.33333333333333\n",
      "Learning rate value: 0.001, Accuracy % = 29.629629629629626\n",
      "Learning rate value: 0.05, Accuracy % = 33.33333333333333\n",
      "Learning rate value: 0.01, Accuracy % = 33.33333333333333\n",
      "Learning rate value: 0.1, Accuracy % = 33.33333333333333\n",
      "Learning rate value: 0.15, Accuracy % = 37.03703703703704\n",
      "End of iteration \n",
      "\n",
      "Lambda = 10\n",
      "Learning rate value: 0.0001, Accuracy % = 33.33333333333333\n",
      "Learning rate value: 0.0005, Accuracy % = 33.33333333333333\n",
      "Learning rate value: 0.001, Accuracy % = 29.629629629629626\n",
      "Learning rate value: 0.05, Accuracy % = 33.33333333333333\n",
      "Learning rate value: 0.01, Accuracy % = 33.33333333333333\n",
      "Learning rate value: 0.1, Accuracy % = 33.33333333333333\n",
      "Learning rate value: 0.15, Accuracy % = 33.33333333333333\n",
      "End of iteration \n",
      "\n",
      "Lambda = 20\n",
      "Learning rate value: 0.0001, Accuracy % = 33.33333333333333\n",
      "Learning rate value: 0.0005, Accuracy % = 33.33333333333333\n",
      "Learning rate value: 0.001, Accuracy % = 29.629629629629626\n",
      "Learning rate value: 0.05, Accuracy % = 33.33333333333333\n",
      "Learning rate value: 0.01, Accuracy % = 33.33333333333333\n",
      "Learning rate value: 0.1, Accuracy % = 33.33333333333333\n",
      "Learning rate value: 0.15, Accuracy % = 33.33333333333333\n",
      "End of iteration \n",
      "\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.0001, 0.0005, 0.0010, 0.050, 0.0100, 0.100, 0.150] \n",
    "lambdas = [0,0.001,0.005,0.01,0.05,0.1,0.5,1,5,10,20]\n",
    "epochs = 100000\n",
    "\n",
    "for lambda1 in lambdas:\n",
    "    print(\"Lambda = \" + str(lambda1))\n",
    "    for alpha in alphas:\n",
    "        theta_x = gradient_descent(X_train,Y_train,weights,alpha,lambda1,epochs)\n",
    "        hypothesis = sigmoid(X_test@theta_x.T)\n",
    "        hypothesis = (hypothesis>=0.5)*1\n",
    "        hypothesis = hypothesis.reshape(-1)\n",
    "        truth = hypothesis==Y_test\n",
    "        acc = (np.sum(truth)/Y_test.shape[0])*100\n",
    "        print(\"Learning rate value: \" + str(alpha) + \", Accuracy % = \" + str(acc))\n",
    "    print(\"End of iteration \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONCLUSION\n",
    "\n",
    "Thus, after observing the process of gradient descent with or without regularisation, we obtain an accuracy of a maximum of 92% using normalisation. but adding a regularising term does not alter the prediction accuracy in any meaningful way. This accuracy is around 5% better than gradient descent without normalisation but with regularisation, which was 87%. Hence, we establish that we get a better accuracy with normalisation. The decision boundary is extremely close to the actual boundary between the passed and failed students in this particular case. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
