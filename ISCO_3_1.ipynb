{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3, Question 2, ISCO630E \n",
    "### Submitted by Bhanu Bhandari (IEC2016027)\n",
    "#### Logistic Regression to predict the passing or failure of components based on two quality tests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = pd.read_csv('/Users/bhanubhandari/Desktop/GCN_3/data_quality.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parameter1</th>\n",
       "      <th>parameter2</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.051267</td>\n",
       "      <td>0.699560</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.092742</td>\n",
       "      <td>0.684940</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.213710</td>\n",
       "      <td>0.692250</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.502190</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.513250</td>\n",
       "      <td>0.465640</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.524770</td>\n",
       "      <td>0.209800</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.398040</td>\n",
       "      <td>0.034357</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.305880</td>\n",
       "      <td>-0.192250</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.016705</td>\n",
       "      <td>-0.404240</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.131910</td>\n",
       "      <td>-0.513890</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.385370</td>\n",
       "      <td>-0.565060</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.529380</td>\n",
       "      <td>-0.521200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.638820</td>\n",
       "      <td>-0.243420</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.736750</td>\n",
       "      <td>-0.184940</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.546660</td>\n",
       "      <td>0.487570</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.322000</td>\n",
       "      <td>0.582600</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.166470</td>\n",
       "      <td>0.538740</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.046659</td>\n",
       "      <td>0.816520</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.173390</td>\n",
       "      <td>0.699560</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.478690</td>\n",
       "      <td>0.633770</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.605410</td>\n",
       "      <td>0.597220</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.628460</td>\n",
       "      <td>0.334060</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.593890</td>\n",
       "      <td>0.005117</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.421080</td>\n",
       "      <td>-0.272660</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.115780</td>\n",
       "      <td>-0.396930</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.201040</td>\n",
       "      <td>-0.601610</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.466010</td>\n",
       "      <td>-0.535820</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.673390</td>\n",
       "      <td>-0.535820</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.138820</td>\n",
       "      <td>0.546050</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.294350</td>\n",
       "      <td>0.779970</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>-0.403800</td>\n",
       "      <td>0.706870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>-0.380760</td>\n",
       "      <td>0.918860</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>-0.507490</td>\n",
       "      <td>0.904240</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>-0.547810</td>\n",
       "      <td>0.706870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.103110</td>\n",
       "      <td>0.779970</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.057028</td>\n",
       "      <td>0.918860</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>-0.104260</td>\n",
       "      <td>0.991960</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-0.081221</td>\n",
       "      <td>1.108900</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.287440</td>\n",
       "      <td>1.087000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.396890</td>\n",
       "      <td>0.823830</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.638820</td>\n",
       "      <td>0.889620</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.823160</td>\n",
       "      <td>0.663010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.673390</td>\n",
       "      <td>0.641080</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1.070900</td>\n",
       "      <td>0.100150</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>-0.046659</td>\n",
       "      <td>-0.579680</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>-0.236750</td>\n",
       "      <td>-0.638160</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>-0.150350</td>\n",
       "      <td>-0.367690</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>-0.490210</td>\n",
       "      <td>-0.301900</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>-0.467170</td>\n",
       "      <td>-0.133770</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>-0.288590</td>\n",
       "      <td>-0.060673</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>-0.611180</td>\n",
       "      <td>-0.067982</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>-0.663020</td>\n",
       "      <td>-0.214180</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>-0.599650</td>\n",
       "      <td>-0.418860</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>-0.726380</td>\n",
       "      <td>-0.082602</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>-0.830070</td>\n",
       "      <td>0.312130</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>-0.720620</td>\n",
       "      <td>0.538740</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>-0.593890</td>\n",
       "      <td>0.494880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>-0.484450</td>\n",
       "      <td>0.999270</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>-0.006336</td>\n",
       "      <td>0.999270</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.632650</td>\n",
       "      <td>-0.030612</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>118 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     parameter1  parameter2  result\n",
       "0      0.051267    0.699560       1\n",
       "1     -0.092742    0.684940       1\n",
       "2     -0.213710    0.692250       1\n",
       "3     -0.375000    0.502190       1\n",
       "4     -0.513250    0.465640       1\n",
       "5     -0.524770    0.209800       1\n",
       "6     -0.398040    0.034357       1\n",
       "7     -0.305880   -0.192250       1\n",
       "8      0.016705   -0.404240       1\n",
       "9      0.131910   -0.513890       1\n",
       "10     0.385370   -0.565060       1\n",
       "11     0.529380   -0.521200       1\n",
       "12     0.638820   -0.243420       1\n",
       "13     0.736750   -0.184940       1\n",
       "14     0.546660    0.487570       1\n",
       "15     0.322000    0.582600       1\n",
       "16     0.166470    0.538740       1\n",
       "17    -0.046659    0.816520       1\n",
       "18    -0.173390    0.699560       1\n",
       "19    -0.478690    0.633770       1\n",
       "20    -0.605410    0.597220       1\n",
       "21    -0.628460    0.334060       1\n",
       "22    -0.593890    0.005117       1\n",
       "23    -0.421080   -0.272660       1\n",
       "24    -0.115780   -0.396930       1\n",
       "25     0.201040   -0.601610       1\n",
       "26     0.466010   -0.535820       1\n",
       "27     0.673390   -0.535820       1\n",
       "28    -0.138820    0.546050       1\n",
       "29    -0.294350    0.779970       1\n",
       "..          ...         ...     ...\n",
       "88    -0.403800    0.706870       0\n",
       "89    -0.380760    0.918860       0\n",
       "90    -0.507490    0.904240       0\n",
       "91    -0.547810    0.706870       0\n",
       "92     0.103110    0.779970       0\n",
       "93     0.057028    0.918860       0\n",
       "94    -0.104260    0.991960       0\n",
       "95    -0.081221    1.108900       0\n",
       "96     0.287440    1.087000       0\n",
       "97     0.396890    0.823830       0\n",
       "98     0.638820    0.889620       0\n",
       "99     0.823160    0.663010       0\n",
       "100    0.673390    0.641080       0\n",
       "101    1.070900    0.100150       0\n",
       "102   -0.046659   -0.579680       0\n",
       "103   -0.236750   -0.638160       0\n",
       "104   -0.150350   -0.367690       0\n",
       "105   -0.490210   -0.301900       0\n",
       "106   -0.467170   -0.133770       0\n",
       "107   -0.288590   -0.060673       0\n",
       "108   -0.611180   -0.067982       0\n",
       "109   -0.663020   -0.214180       0\n",
       "110   -0.599650   -0.418860       0\n",
       "111   -0.726380   -0.082602       0\n",
       "112   -0.830070    0.312130       0\n",
       "113   -0.720620    0.538740       0\n",
       "114   -0.593890    0.494880       0\n",
       "115   -0.484450    0.999270       0\n",
       "116   -0.006336    0.999270       0\n",
       "117    0.632650   -0.030612       0\n",
       "\n",
       "[118 rows x 3 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = input_1[[\"parameter1\",\"parameter2\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parameter1</th>\n",
       "      <th>parameter2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.051267</td>\n",
       "      <td>0.699560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.092742</td>\n",
       "      <td>0.684940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.213710</td>\n",
       "      <td>0.692250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.502190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.513250</td>\n",
       "      <td>0.465640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.524770</td>\n",
       "      <td>0.209800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.398040</td>\n",
       "      <td>0.034357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.305880</td>\n",
       "      <td>-0.192250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.016705</td>\n",
       "      <td>-0.404240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.131910</td>\n",
       "      <td>-0.513890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.385370</td>\n",
       "      <td>-0.565060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.529380</td>\n",
       "      <td>-0.521200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.638820</td>\n",
       "      <td>-0.243420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.736750</td>\n",
       "      <td>-0.184940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.546660</td>\n",
       "      <td>0.487570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.322000</td>\n",
       "      <td>0.582600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.166470</td>\n",
       "      <td>0.538740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.046659</td>\n",
       "      <td>0.816520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.173390</td>\n",
       "      <td>0.699560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.478690</td>\n",
       "      <td>0.633770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.605410</td>\n",
       "      <td>0.597220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.628460</td>\n",
       "      <td>0.334060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.593890</td>\n",
       "      <td>0.005117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.421080</td>\n",
       "      <td>-0.272660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.115780</td>\n",
       "      <td>-0.396930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.201040</td>\n",
       "      <td>-0.601610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.466010</td>\n",
       "      <td>-0.535820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.673390</td>\n",
       "      <td>-0.535820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.138820</td>\n",
       "      <td>0.546050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.294350</td>\n",
       "      <td>0.779970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>-0.403800</td>\n",
       "      <td>0.706870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>-0.380760</td>\n",
       "      <td>0.918860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>-0.507490</td>\n",
       "      <td>0.904240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>-0.547810</td>\n",
       "      <td>0.706870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.103110</td>\n",
       "      <td>0.779970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.057028</td>\n",
       "      <td>0.918860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>-0.104260</td>\n",
       "      <td>0.991960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-0.081221</td>\n",
       "      <td>1.108900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.287440</td>\n",
       "      <td>1.087000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.396890</td>\n",
       "      <td>0.823830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.638820</td>\n",
       "      <td>0.889620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.823160</td>\n",
       "      <td>0.663010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.673390</td>\n",
       "      <td>0.641080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1.070900</td>\n",
       "      <td>0.100150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>-0.046659</td>\n",
       "      <td>-0.579680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>-0.236750</td>\n",
       "      <td>-0.638160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>-0.150350</td>\n",
       "      <td>-0.367690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>-0.490210</td>\n",
       "      <td>-0.301900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>-0.467170</td>\n",
       "      <td>-0.133770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>-0.288590</td>\n",
       "      <td>-0.060673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>-0.611180</td>\n",
       "      <td>-0.067982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>-0.663020</td>\n",
       "      <td>-0.214180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>-0.599650</td>\n",
       "      <td>-0.418860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>-0.726380</td>\n",
       "      <td>-0.082602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>-0.830070</td>\n",
       "      <td>0.312130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>-0.720620</td>\n",
       "      <td>0.538740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>-0.593890</td>\n",
       "      <td>0.494880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>-0.484450</td>\n",
       "      <td>0.999270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>-0.006336</td>\n",
       "      <td>0.999270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.632650</td>\n",
       "      <td>-0.030612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>118 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     parameter1  parameter2\n",
       "0      0.051267    0.699560\n",
       "1     -0.092742    0.684940\n",
       "2     -0.213710    0.692250\n",
       "3     -0.375000    0.502190\n",
       "4     -0.513250    0.465640\n",
       "5     -0.524770    0.209800\n",
       "6     -0.398040    0.034357\n",
       "7     -0.305880   -0.192250\n",
       "8      0.016705   -0.404240\n",
       "9      0.131910   -0.513890\n",
       "10     0.385370   -0.565060\n",
       "11     0.529380   -0.521200\n",
       "12     0.638820   -0.243420\n",
       "13     0.736750   -0.184940\n",
       "14     0.546660    0.487570\n",
       "15     0.322000    0.582600\n",
       "16     0.166470    0.538740\n",
       "17    -0.046659    0.816520\n",
       "18    -0.173390    0.699560\n",
       "19    -0.478690    0.633770\n",
       "20    -0.605410    0.597220\n",
       "21    -0.628460    0.334060\n",
       "22    -0.593890    0.005117\n",
       "23    -0.421080   -0.272660\n",
       "24    -0.115780   -0.396930\n",
       "25     0.201040   -0.601610\n",
       "26     0.466010   -0.535820\n",
       "27     0.673390   -0.535820\n",
       "28    -0.138820    0.546050\n",
       "29    -0.294350    0.779970\n",
       "..          ...         ...\n",
       "88    -0.403800    0.706870\n",
       "89    -0.380760    0.918860\n",
       "90    -0.507490    0.904240\n",
       "91    -0.547810    0.706870\n",
       "92     0.103110    0.779970\n",
       "93     0.057028    0.918860\n",
       "94    -0.104260    0.991960\n",
       "95    -0.081221    1.108900\n",
       "96     0.287440    1.087000\n",
       "97     0.396890    0.823830\n",
       "98     0.638820    0.889620\n",
       "99     0.823160    0.663010\n",
       "100    0.673390    0.641080\n",
       "101    1.070900    0.100150\n",
       "102   -0.046659   -0.579680\n",
       "103   -0.236750   -0.638160\n",
       "104   -0.150350   -0.367690\n",
       "105   -0.490210   -0.301900\n",
       "106   -0.467170   -0.133770\n",
       "107   -0.288590   -0.060673\n",
       "108   -0.611180   -0.067982\n",
       "109   -0.663020   -0.214180\n",
       "110   -0.599650   -0.418860\n",
       "111   -0.726380   -0.082602\n",
       "112   -0.830070    0.312130\n",
       "113   -0.720620    0.538740\n",
       "114   -0.593890    0.494880\n",
       "115   -0.484450    0.999270\n",
       "116   -0.006336    0.999270\n",
       "117    0.632650   -0.030612\n",
       "\n",
       "[118 rows x 2 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = input_1[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we shall visualise the entire dataset in order to observe the tendency and distribution of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 2) (118,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(X.shape, Y.shape)\n",
    "for i in range(X.shape[0]):\n",
    "    if Y[i]==1:\n",
    "        plt.plot(X[i,0],X[i,1],'rx')\n",
    "    else:\n",
    "        plt.plot(X[i,0],X[i,1],'gx')\n",
    "plt.xlabel('Parameter 1')\n",
    "plt.ylabel('Parameter 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we observe that the boundary does not resemble a linear shape. However, to start off the analysis, we use the linear hypothesis: \n",
    "\n",
    "## $$ y = w_0 + w_1x_1 + w_2x_2  $$\n",
    "\n",
    "## Train-test splitting\n",
    "\n",
    "Before we begin with the training and testing process, we first define completely random choices for the training and the testing datasets with a 70:30 split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79, 4) (39, 4) (86,) (32,)\n"
     ]
    }
   ],
   "source": [
    "random_val_msk = np.random.rand(len(X)) < 0.7\n",
    "X_train = X[random_val_msk]\n",
    "X_test = X[~random_val_msk]\n",
    "Y_train = Y[random_val_msk]\n",
    "Y_test = Y[~random_val_msk] \n",
    "print(X1_train.shape, X1_test.shape, Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.43267994 0.78039935]\n",
      " [0.35255386 0.77255609]\n",
      " [0.28524771 0.77647772]\n",
      " [0.11217506 0.51765539]\n",
      " [0.18268717 0.42353462]\n",
      " [0.41344977 0.18823832]\n",
      " [0.47754941 0.12941385]\n",
      " [0.69870026 0.12549222]\n",
      " [0.75959227 0.27451422]\n",
      " [0.81408017 0.30588728]\n",
      " [0.70831479 0.66667203]\n",
      " [0.58331479 0.71765324]\n",
      " [0.49677847 0.69412345]\n",
      " [0.37819427 0.84314546]\n",
      " [0.30768161 0.78039935]\n",
      " [0.13781381 0.74510467]\n",
      " [0.06730726 0.72549651]\n",
      " [0.07371695 0.40784809]\n",
      " [0.1698678  0.25882769]\n",
      " [0.3397356  0.19215996]\n",
      " [0.51601309 0.08235427]\n",
      " [0.32691623 0.69804509]\n",
      " [0.24037991 0.8235373 ]\n",
      " [0.25640412 0.92157809]\n",
      " [0.30768161 0.75294793]\n",
      " [0.24678959 0.65882877]\n",
      " [0.20191623 0.57255287]\n",
      " [0.36857919 0.30588728]\n",
      " [0.43909018 0.31765217]\n",
      " [0.5320373  0.18431669]\n",
      " [0.56729057 0.28235749]\n",
      " [0.67305595 0.30588728]\n",
      " [0.76280268 0.32941707]\n",
      " [0.52883246 0.68628019]\n",
      " [0.65062205 0.76471282]\n",
      " [0.58331479 0.77647772]\n",
      " [0.48075425 0.71373161]\n",
      " [0.40062962 0.61961245]\n",
      " [0.35255386 0.70196672]\n",
      " [0.28845255 0.59608266]\n",
      " [0.28845255 0.49804723]\n",
      " [0.16025327 0.52157702]\n",
      " [0.28204286 0.3960832 ]\n",
      " [0.32691623 0.25882769]\n",
      " [0.50639856 0.90589157]\n",
      " [0.52883246 0.8235373 ]\n",
      " [0.57049542 0.7372614 ]\n",
      " [0.68588089 0.81177241]\n",
      " [0.7467729  0.79608588]\n",
      " [0.83010438 0.67451529]\n",
      " [0.91984554 0.60000429]\n",
      " [0.86215837 0.55294471]\n",
      " [0.93908017 0.4509855 ]\n",
      " [0.92625523 0.41176972]\n",
      " [0.90382133 0.29412238]\n",
      " [0.86536322 0.12549222]\n",
      " [0.73395353 0.00784326]\n",
      " [0.69229057 0.0862759 ]\n",
      " [0.66344142 0.18039506]\n",
      " [0.59934457 0.09411916]\n",
      " [0.45191011 0.        ]\n",
      " [0.48716394 0.09411916]\n",
      " [0.33012107 0.16470853]\n",
      " [0.18589201 0.26667096]\n",
      " [0.01602421 0.42745625]\n",
      " [0.01602421 0.77255609]\n",
      " [0.1923017  0.8980483 ]\n",
      " [0.12178959 0.89020504]\n",
      " [0.46152519 0.8235373 ]\n",
      " [0.34614529 0.93726462]\n",
      " [0.3589641  1.        ]\n",
      " [0.62498331 0.84706709]\n",
      " [0.75959227 0.88236178]\n",
      " [0.86215837 0.76079119]\n",
      " [1.         0.45883091]\n",
      " [0.27242834 0.06274611]\n",
      " [0.32050098 0.20784648]\n",
      " [0.13140412 0.24314117]\n",
      " [0.14422349 0.3333387 ]\n",
      " [0.24358475 0.37255341]\n",
      " [0.06409686 0.36863231]\n",
      " [0.         0.36078905]\n",
      " [0.00320484 0.69412345]\n",
      " [0.07371695 0.67059366]\n",
      " [0.13460896 0.94118625]\n",
      " [0.75615931 0.38868038]] [[0.26870774 0.68503398]\n",
      " [0.18707449 0.66534894]\n",
      " [0.30952142 0.31102362]\n",
      " [0.7176877  0.11023622]\n",
      " [0.11904579 0.5944828 ]\n",
      " [0.76530365 0.12598425]\n",
      " [0.88775649 0.12598425]\n",
      " [0.39455581 0.84645131]\n",
      " [0.31292256 0.42913224]\n",
      " [0.350341   0.2992126 ]\n",
      " [0.76190251 0.42125823]\n",
      " [0.86054147 0.5       ]\n",
      " [0.82993121 0.55904973]\n",
      " [0.91836674 0.65353792]\n",
      " [0.84693691 0.73621509]\n",
      " [1.         0.37007766]\n",
      " [0.99319772 0.21653543]\n",
      " [0.95918042 0.11417323]\n",
      " [0.65986242 0.        ]\n",
      " [0.24830091 0.19291339]\n",
      " [0.051023   0.27559055]\n",
      " [0.04422072 0.57086076]\n",
      " [0.25170205 0.7952702 ]\n",
      " [0.16666765 0.7952702 ]\n",
      " [0.52380975 0.90944343]\n",
      " [0.65986242 1.        ]\n",
      " [0.88775649 0.75983713]\n",
      " [0.4625851  0.1023622 ]\n",
      " [0.09863895 0.2992126 ]\n",
      " [0.13605739 0.18897638]\n",
      " [0.         0.58267178]\n",
      " [0.48639461 0.95275052]]\n"
     ]
    }
   ],
   "source": [
    "X_train = (X_train-np.min(X_train,axis=0))/(np.max(X_train,axis=0)-np.min(X_train,axis=0))\n",
    "X_test = (X_test-np.min(X_test,axis=0))/(np.max(X_test,axis=0)-np.min(X_test,axis=0))\n",
    "print(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we must add the term for ensuring the bias term in the hypothesis, and we add this to both the training and the testing subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86, 3)\n"
     ]
    }
   ],
   "source": [
    "# Adding row for bias term w0 in the hypothesis to the entire dataset \n",
    "X_train = np.concatenate( (np.ones((X_train.shape[0],1)),X_train) , axis=1)\n",
    "X_test = np.concatenate( (np.ones((X_test.shape[0],1)),X_test) , axis=1)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now visualise the training set again after normalisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(X_train.shape[0]):\n",
    "    if Y_train[i]==1:\n",
    "        plt.plot(X_train[i,1],X_train[i,2],'rx')\n",
    "    else:\n",
    "        plt.plot(X_train[i,1],X_train[i,2],'gx')\n",
    "plt.xlabel('Parameter 1 - Normalised')\n",
    "plt.ylabel('Parameter 2 - Normalised')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the data is not very linearly separated in this case. (Green for pass, Red for fail). However, we begin with a linear decision boundary of the form: \n",
    "## $$ y = w_0 + w_1x_1 + w_2x_2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Sigmoid Function\n",
    "Before we proceed with **training**, we must declare our sigmoid and sigmoid derivative functions, to be used later in the training process. We define the sigmoid function as: \n",
    "\n",
    "## $$ g(x) = 1/(1 + e^{-x}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now intialise the weights randomly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.46211306 0.46159024 0.66240971]]\n",
      "(86,)\n"
     ]
    }
   ],
   "source": [
    "m,n = X_train.shape # n is the shape of original_X.shape[0]+1\n",
    "weights = np.random.rand(1,n)\n",
    "print(weights)\n",
    "h = sigmoid(X_train@weights.T)\n",
    "h = h.reshape(-1)\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X,Y,theta,alpha,lambda1,iterations):\n",
    "    i=0\n",
    "    while(i<iterations):\n",
    "        sigmoid_output = sigmoid(X@theta.T).reshape(-1)\n",
    "        theta = theta - (alpha/m)*(X.T@(sigmoid_output-Y)) - (lambda1/m)*np.sum(theta[:,1:])\n",
    "        i+=1\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate value = 0.0001, Accuracy % = 56.25\n",
      "Learning rate value = 0.0005, Accuracy % = 43.75\n",
      "Learning rate value = 0.001, Accuracy % = 46.875\n",
      "Learning rate value = 0.05, Accuracy % = 46.875\n",
      "Learning rate value = 0.01, Accuracy % = 46.875\n",
      "Learning rate value = 0.1, Accuracy % = 46.875\n",
      "Learning rate value = 0.15, Accuracy % = 46.875\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.0001, 0.0005, 0.0010, 0.050, 0.0100, 0.100, 0.150] \n",
    "epochs = 100000\n",
    "\n",
    "for alpha in alphas:\n",
    "    theta_x = gradient_descent(X_train,Y_train,weights,alpha,0,epochs) \n",
    "    h = sigmoid(X_test@theta_x.T)\n",
    "    h = ((h>=0.5)*1).reshape(-1)\n",
    "    accuracy = (np.sum(h==Y_test)/Y_test.shape[0])*100\n",
    "    print(\"Learning rate value = \" + str(alpha) + \", Accuracy % = \" + str(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda = 0\n",
      "Learning rate value: 0.0001, Accuracy % = 46.875\n",
      "Learning rate value: 0.0005, Accuracy % = 46.875\n",
      "Learning rate value: 0.001, Accuracy % = 46.875\n",
      "Learning rate value: 0.05, Accuracy % = 46.875\n",
      "Learning rate value: 0.01, Accuracy % = 46.875\n",
      "Learning rate value: 0.1, Accuracy % = 46.875\n",
      "Learning rate value: 0.15, Accuracy % = 46.875\n",
      "End of iteration \n",
      "\n",
      "Lambda = 0.001\n",
      "Learning rate value: 0.0001, Accuracy % = 46.875\n",
      "Learning rate value: 0.0005, Accuracy % = 46.875\n",
      "Learning rate value: 0.001, Accuracy % = 46.875\n",
      "Learning rate value: 0.05, Accuracy % = 46.875\n",
      "Learning rate value: 0.01, Accuracy % = 46.875\n",
      "Learning rate value: 0.1, Accuracy % = 46.875\n",
      "Learning rate value: 0.15, Accuracy % = 46.875\n",
      "End of iteration \n",
      "\n",
      "Lambda = 0.005\n",
      "Learning rate value: 0.0001, Accuracy % = 46.875\n",
      "Learning rate value: 0.0005, Accuracy % = 46.875\n",
      "Learning rate value: 0.001, Accuracy % = 46.875\n",
      "Learning rate value: 0.05, Accuracy % = 46.875\n",
      "Learning rate value: 0.01, Accuracy % = 46.875\n",
      "Learning rate value: 0.1, Accuracy % = 46.875\n",
      "Learning rate value: 0.15, Accuracy % = 46.875\n",
      "End of iteration \n",
      "\n",
      "Lambda = 0.01\n",
      "Learning rate value: 0.0001, Accuracy % = 46.875\n",
      "Learning rate value: 0.0005, Accuracy % = 46.875\n",
      "Learning rate value: 0.001, Accuracy % = 46.875\n",
      "Learning rate value: 0.05, Accuracy % = 46.875\n",
      "Learning rate value: 0.01, Accuracy % = 46.875\n",
      "Learning rate value: 0.1, Accuracy % = 46.875\n",
      "Learning rate value: 0.15, Accuracy % = 46.875\n",
      "End of iteration \n",
      "\n",
      "Lambda = 0.05\n",
      "Learning rate value: 0.0001, Accuracy % = 46.875\n",
      "Learning rate value: 0.0005, Accuracy % = 46.875\n",
      "Learning rate value: 0.001, Accuracy % = 46.875\n",
      "Learning rate value: 0.05, Accuracy % = 46.875\n",
      "Learning rate value: 0.01, Accuracy % = 46.875\n",
      "Learning rate value: 0.1, Accuracy % = 46.875\n",
      "Learning rate value: 0.15, Accuracy % = 46.875\n",
      "End of iteration \n",
      "\n",
      "Lambda = 0.1\n",
      "Learning rate value: 0.0001, Accuracy % = 46.875\n",
      "Learning rate value: 0.0005, Accuracy % = 46.875\n",
      "Learning rate value: 0.001, Accuracy % = 46.875\n",
      "Learning rate value: 0.05, Accuracy % = 46.875\n",
      "Learning rate value: 0.01, Accuracy % = 46.875\n",
      "Learning rate value: 0.1, Accuracy % = 46.875\n",
      "Learning rate value: 0.15, Accuracy % = 46.875\n",
      "End of iteration \n",
      "\n",
      "Lambda = 0.5\n",
      "Learning rate value: 0.0001, Accuracy % = 46.875\n",
      "Learning rate value: 0.0005, Accuracy % = 46.875\n",
      "Learning rate value: 0.001, Accuracy % = 46.875\n",
      "Learning rate value: 0.05, Accuracy % = 46.875\n",
      "Learning rate value: 0.01, Accuracy % = 46.875\n",
      "Learning rate value: 0.1, Accuracy % = 46.875\n",
      "Learning rate value: 0.15, Accuracy % = 46.875\n",
      "End of iteration \n",
      "\n",
      "Lambda = 1\n",
      "Learning rate value: 0.0001, Accuracy % = 46.875\n",
      "Learning rate value: 0.0005, Accuracy % = 46.875\n",
      "Learning rate value: 0.001, Accuracy % = 46.875\n",
      "Learning rate value: 0.05, Accuracy % = 46.875\n",
      "Learning rate value: 0.01, Accuracy % = 46.875\n",
      "Learning rate value: 0.1, Accuracy % = 46.875\n",
      "Learning rate value: 0.15, Accuracy % = 46.875\n",
      "End of iteration \n",
      "\n",
      "Lambda = 5\n",
      "Learning rate value: 0.0001, Accuracy % = 46.875\n",
      "Learning rate value: 0.0005, Accuracy % = 46.875\n",
      "Learning rate value: 0.001, Accuracy % = 46.875\n",
      "Learning rate value: 0.05, Accuracy % = 46.875\n",
      "Learning rate value: 0.01, Accuracy % = 46.875\n",
      "Learning rate value: 0.1, Accuracy % = 46.875\n",
      "Learning rate value: 0.15, Accuracy % = 46.875\n",
      "End of iteration \n",
      "\n",
      "Lambda = 10\n",
      "Learning rate value: 0.0001, Accuracy % = 46.875\n",
      "Learning rate value: 0.0005, Accuracy % = 46.875\n",
      "Learning rate value: 0.001, Accuracy % = 46.875\n",
      "Learning rate value: 0.05, Accuracy % = 46.875\n",
      "Learning rate value: 0.01, Accuracy % = 46.875\n",
      "Learning rate value: 0.1, Accuracy % = 46.875\n",
      "Learning rate value: 0.15, Accuracy % = 46.875\n",
      "End of iteration \n",
      "\n",
      "Lambda = 20\n",
      "Learning rate value: 0.0001, Accuracy % = 46.875\n",
      "Learning rate value: 0.0005, Accuracy % = 46.875\n",
      "Learning rate value: 0.001, Accuracy % = 46.875\n",
      "Learning rate value: 0.05, Accuracy % = 46.875\n",
      "Learning rate value: 0.01, Accuracy % = 46.875\n",
      "Learning rate value: 0.1, Accuracy % = 46.875\n",
      "Learning rate value: 0.15, Accuracy % = 46.875\n",
      "End of iteration \n",
      "\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.0001, 0.0005, 0.0010, 0.050, 0.0100, 0.100, 0.150] \n",
    "lambdas = [0,0.001,0.005,0.01,0.05,0.1,0.5,1,5,10,20]\n",
    "epochs = 100000\n",
    "\n",
    "for lambda1 in lambdas:\n",
    "    print(\"Lambda = \" + str(lambda1))\n",
    "    for alpha in alphas:\n",
    "        theta_x = gradient_descent(X_train,Y_train,weights,alpha,lambda1,epochs)\n",
    "        hypothesis = sigmoid(X_test@theta_x.T)\n",
    "        hypothesis = (hypothesis>=0.5)*1\n",
    "        hypothesis = hypothesis.reshape(-1)\n",
    "        truth = h==Y_test\n",
    "        acc = (np.sum(truth)/Y_test.shape[0])*100\n",
    "        print(\"Learning rate value: \" + str(alpha) + \", Accuracy % = \" + str(acc))\n",
    "    print(\"End of iteration \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates that a linear hypothesis is not able to predict satisfactorily for features as are present in this dataset. We are getting a mediocre accuracy, even after regularisation. \n",
    "\n",
    "Hence, we need to extract higher order features as well. We now use the circular boundary hypothesis instead of a linear one: \n",
    "## $$ y = w_0 + w_1x_1 + w_2x_2 + w_3x_1^2 + w_4x_2^2 $$ \n",
    "And in order to achieve this, we need to append these higher order features to the original training inputs, and hence modify the training and testing sets. We also run the above analysis on these higher order features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = input_1[[\"parameter1\",\"parameter2\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1[\"parameter3\"] = input_1[[\"parameter1\"]]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1[\"parameter4\"] = input_1[[\"parameter2\"]]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parameter1</th>\n",
       "      <th>parameter2</th>\n",
       "      <th>parameter3</th>\n",
       "      <th>parameter4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.051267</td>\n",
       "      <td>0.699560</td>\n",
       "      <td>0.002628</td>\n",
       "      <td>0.489384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.092742</td>\n",
       "      <td>0.684940</td>\n",
       "      <td>0.008601</td>\n",
       "      <td>0.469143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.213710</td>\n",
       "      <td>0.692250</td>\n",
       "      <td>0.045672</td>\n",
       "      <td>0.479210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.502190</td>\n",
       "      <td>0.140625</td>\n",
       "      <td>0.252195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.513250</td>\n",
       "      <td>0.465640</td>\n",
       "      <td>0.263426</td>\n",
       "      <td>0.216821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.524770</td>\n",
       "      <td>0.209800</td>\n",
       "      <td>0.275384</td>\n",
       "      <td>0.044016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.398040</td>\n",
       "      <td>0.034357</td>\n",
       "      <td>0.158436</td>\n",
       "      <td>0.001180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.305880</td>\n",
       "      <td>-0.192250</td>\n",
       "      <td>0.093563</td>\n",
       "      <td>0.036960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.016705</td>\n",
       "      <td>-0.404240</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.163410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.131910</td>\n",
       "      <td>-0.513890</td>\n",
       "      <td>0.017400</td>\n",
       "      <td>0.264083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.385370</td>\n",
       "      <td>-0.565060</td>\n",
       "      <td>0.148510</td>\n",
       "      <td>0.319293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.529380</td>\n",
       "      <td>-0.521200</td>\n",
       "      <td>0.280243</td>\n",
       "      <td>0.271649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.638820</td>\n",
       "      <td>-0.243420</td>\n",
       "      <td>0.408091</td>\n",
       "      <td>0.059253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.736750</td>\n",
       "      <td>-0.184940</td>\n",
       "      <td>0.542801</td>\n",
       "      <td>0.034203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.546660</td>\n",
       "      <td>0.487570</td>\n",
       "      <td>0.298837</td>\n",
       "      <td>0.237725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.322000</td>\n",
       "      <td>0.582600</td>\n",
       "      <td>0.103684</td>\n",
       "      <td>0.339423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.166470</td>\n",
       "      <td>0.538740</td>\n",
       "      <td>0.027712</td>\n",
       "      <td>0.290241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.046659</td>\n",
       "      <td>0.816520</td>\n",
       "      <td>0.002177</td>\n",
       "      <td>0.666705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.173390</td>\n",
       "      <td>0.699560</td>\n",
       "      <td>0.030064</td>\n",
       "      <td>0.489384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.478690</td>\n",
       "      <td>0.633770</td>\n",
       "      <td>0.229144</td>\n",
       "      <td>0.401664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.605410</td>\n",
       "      <td>0.597220</td>\n",
       "      <td>0.366521</td>\n",
       "      <td>0.356672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.628460</td>\n",
       "      <td>0.334060</td>\n",
       "      <td>0.394962</td>\n",
       "      <td>0.111596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.593890</td>\n",
       "      <td>0.005117</td>\n",
       "      <td>0.352705</td>\n",
       "      <td>0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.421080</td>\n",
       "      <td>-0.272660</td>\n",
       "      <td>0.177308</td>\n",
       "      <td>0.074343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.115780</td>\n",
       "      <td>-0.396930</td>\n",
       "      <td>0.013405</td>\n",
       "      <td>0.157553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.201040</td>\n",
       "      <td>-0.601610</td>\n",
       "      <td>0.040417</td>\n",
       "      <td>0.361935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.466010</td>\n",
       "      <td>-0.535820</td>\n",
       "      <td>0.217165</td>\n",
       "      <td>0.287103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.673390</td>\n",
       "      <td>-0.535820</td>\n",
       "      <td>0.453454</td>\n",
       "      <td>0.287103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.138820</td>\n",
       "      <td>0.546050</td>\n",
       "      <td>0.019271</td>\n",
       "      <td>0.298171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.294350</td>\n",
       "      <td>0.779970</td>\n",
       "      <td>0.086642</td>\n",
       "      <td>0.608353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>-0.403800</td>\n",
       "      <td>0.706870</td>\n",
       "      <td>0.163054</td>\n",
       "      <td>0.499665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>-0.380760</td>\n",
       "      <td>0.918860</td>\n",
       "      <td>0.144978</td>\n",
       "      <td>0.844304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>-0.507490</td>\n",
       "      <td>0.904240</td>\n",
       "      <td>0.257546</td>\n",
       "      <td>0.817650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>-0.547810</td>\n",
       "      <td>0.706870</td>\n",
       "      <td>0.300096</td>\n",
       "      <td>0.499665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.103110</td>\n",
       "      <td>0.779970</td>\n",
       "      <td>0.010632</td>\n",
       "      <td>0.608353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.057028</td>\n",
       "      <td>0.918860</td>\n",
       "      <td>0.003252</td>\n",
       "      <td>0.844304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>-0.104260</td>\n",
       "      <td>0.991960</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.983985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-0.081221</td>\n",
       "      <td>1.108900</td>\n",
       "      <td>0.006597</td>\n",
       "      <td>1.229659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.287440</td>\n",
       "      <td>1.087000</td>\n",
       "      <td>0.082622</td>\n",
       "      <td>1.181569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.396890</td>\n",
       "      <td>0.823830</td>\n",
       "      <td>0.157522</td>\n",
       "      <td>0.678696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.638820</td>\n",
       "      <td>0.889620</td>\n",
       "      <td>0.408091</td>\n",
       "      <td>0.791424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.823160</td>\n",
       "      <td>0.663010</td>\n",
       "      <td>0.677592</td>\n",
       "      <td>0.439582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.673390</td>\n",
       "      <td>0.641080</td>\n",
       "      <td>0.453454</td>\n",
       "      <td>0.410984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1.070900</td>\n",
       "      <td>0.100150</td>\n",
       "      <td>1.146827</td>\n",
       "      <td>0.010030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>-0.046659</td>\n",
       "      <td>-0.579680</td>\n",
       "      <td>0.002177</td>\n",
       "      <td>0.336029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>-0.236750</td>\n",
       "      <td>-0.638160</td>\n",
       "      <td>0.056051</td>\n",
       "      <td>0.407248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>-0.150350</td>\n",
       "      <td>-0.367690</td>\n",
       "      <td>0.022605</td>\n",
       "      <td>0.135196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>-0.490210</td>\n",
       "      <td>-0.301900</td>\n",
       "      <td>0.240306</td>\n",
       "      <td>0.091144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>-0.467170</td>\n",
       "      <td>-0.133770</td>\n",
       "      <td>0.218248</td>\n",
       "      <td>0.017894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>-0.288590</td>\n",
       "      <td>-0.060673</td>\n",
       "      <td>0.083284</td>\n",
       "      <td>0.003681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>-0.611180</td>\n",
       "      <td>-0.067982</td>\n",
       "      <td>0.373541</td>\n",
       "      <td>0.004622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>-0.663020</td>\n",
       "      <td>-0.214180</td>\n",
       "      <td>0.439596</td>\n",
       "      <td>0.045873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>-0.599650</td>\n",
       "      <td>-0.418860</td>\n",
       "      <td>0.359580</td>\n",
       "      <td>0.175444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>-0.726380</td>\n",
       "      <td>-0.082602</td>\n",
       "      <td>0.527628</td>\n",
       "      <td>0.006823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>-0.830070</td>\n",
       "      <td>0.312130</td>\n",
       "      <td>0.689016</td>\n",
       "      <td>0.097425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>-0.720620</td>\n",
       "      <td>0.538740</td>\n",
       "      <td>0.519293</td>\n",
       "      <td>0.290241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>-0.593890</td>\n",
       "      <td>0.494880</td>\n",
       "      <td>0.352705</td>\n",
       "      <td>0.244906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>-0.484450</td>\n",
       "      <td>0.999270</td>\n",
       "      <td>0.234692</td>\n",
       "      <td>0.998541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>-0.006336</td>\n",
       "      <td>0.999270</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.998541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.632650</td>\n",
       "      <td>-0.030612</td>\n",
       "      <td>0.400246</td>\n",
       "      <td>0.000937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>118 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     parameter1  parameter2  parameter3  parameter4\n",
       "0      0.051267    0.699560    0.002628    0.489384\n",
       "1     -0.092742    0.684940    0.008601    0.469143\n",
       "2     -0.213710    0.692250    0.045672    0.479210\n",
       "3     -0.375000    0.502190    0.140625    0.252195\n",
       "4     -0.513250    0.465640    0.263426    0.216821\n",
       "5     -0.524770    0.209800    0.275384    0.044016\n",
       "6     -0.398040    0.034357    0.158436    0.001180\n",
       "7     -0.305880   -0.192250    0.093563    0.036960\n",
       "8      0.016705   -0.404240    0.000279    0.163410\n",
       "9      0.131910   -0.513890    0.017400    0.264083\n",
       "10     0.385370   -0.565060    0.148510    0.319293\n",
       "11     0.529380   -0.521200    0.280243    0.271649\n",
       "12     0.638820   -0.243420    0.408091    0.059253\n",
       "13     0.736750   -0.184940    0.542801    0.034203\n",
       "14     0.546660    0.487570    0.298837    0.237725\n",
       "15     0.322000    0.582600    0.103684    0.339423\n",
       "16     0.166470    0.538740    0.027712    0.290241\n",
       "17    -0.046659    0.816520    0.002177    0.666705\n",
       "18    -0.173390    0.699560    0.030064    0.489384\n",
       "19    -0.478690    0.633770    0.229144    0.401664\n",
       "20    -0.605410    0.597220    0.366521    0.356672\n",
       "21    -0.628460    0.334060    0.394962    0.111596\n",
       "22    -0.593890    0.005117    0.352705    0.000026\n",
       "23    -0.421080   -0.272660    0.177308    0.074343\n",
       "24    -0.115780   -0.396930    0.013405    0.157553\n",
       "25     0.201040   -0.601610    0.040417    0.361935\n",
       "26     0.466010   -0.535820    0.217165    0.287103\n",
       "27     0.673390   -0.535820    0.453454    0.287103\n",
       "28    -0.138820    0.546050    0.019271    0.298171\n",
       "29    -0.294350    0.779970    0.086642    0.608353\n",
       "..          ...         ...         ...         ...\n",
       "88    -0.403800    0.706870    0.163054    0.499665\n",
       "89    -0.380760    0.918860    0.144978    0.844304\n",
       "90    -0.507490    0.904240    0.257546    0.817650\n",
       "91    -0.547810    0.706870    0.300096    0.499665\n",
       "92     0.103110    0.779970    0.010632    0.608353\n",
       "93     0.057028    0.918860    0.003252    0.844304\n",
       "94    -0.104260    0.991960    0.010870    0.983985\n",
       "95    -0.081221    1.108900    0.006597    1.229659\n",
       "96     0.287440    1.087000    0.082622    1.181569\n",
       "97     0.396890    0.823830    0.157522    0.678696\n",
       "98     0.638820    0.889620    0.408091    0.791424\n",
       "99     0.823160    0.663010    0.677592    0.439582\n",
       "100    0.673390    0.641080    0.453454    0.410984\n",
       "101    1.070900    0.100150    1.146827    0.010030\n",
       "102   -0.046659   -0.579680    0.002177    0.336029\n",
       "103   -0.236750   -0.638160    0.056051    0.407248\n",
       "104   -0.150350   -0.367690    0.022605    0.135196\n",
       "105   -0.490210   -0.301900    0.240306    0.091144\n",
       "106   -0.467170   -0.133770    0.218248    0.017894\n",
       "107   -0.288590   -0.060673    0.083284    0.003681\n",
       "108   -0.611180   -0.067982    0.373541    0.004622\n",
       "109   -0.663020   -0.214180    0.439596    0.045873\n",
       "110   -0.599650   -0.418860    0.359580    0.175444\n",
       "111   -0.726380   -0.082602    0.527628    0.006823\n",
       "112   -0.830070    0.312130    0.689016    0.097425\n",
       "113   -0.720620    0.538740    0.519293    0.290241\n",
       "114   -0.593890    0.494880    0.352705    0.244906\n",
       "115   -0.484450    0.999270    0.234692    0.998541\n",
       "116   -0.006336    0.999270    0.000040    0.998541\n",
       "117    0.632650   -0.030612    0.400246    0.000937\n",
       "\n",
       "[118 rows x 4 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79, 4) (39, 4) (79,) (39,)\n",
      "[[0.86173203 0.23524639 0.36278579 0.97391774]]\n",
      "(36,)\n"
     ]
    }
   ],
   "source": [
    "random_val_msk = np.random.rand(len(X)) < 0.7\n",
    "X1_train = X1[random_val_msk]\n",
    "X1_test = X1[~random_val_msk]\n",
    "Y_train = Y[random_val_msk]\n",
    "Y_test = Y[~random_val_msk] \n",
    "print(X1_train.shape, X1_test.shape, Y_train.shape, Y_test.shape)\n",
    "\n",
    "m1,n1 = X1_train.shape # n is the shape of original_X.shape[0]+1\n",
    "weights_1 = np.random.rand(1,n1)\n",
    "print(weights_1)\n",
    "hyp_1 = sigmoid(X1_train@weights_1.T)\n",
    "hyp_1 = hyp_1.reshape(-1)\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate value = 5e-05, Accuracy % = 46.15384615384615\n",
      "Learning rate value = 6e-05, Accuracy % = 43.58974358974359\n",
      "Learning rate value = 7e-05, Accuracy % = 43.58974358974359\n",
      "Learning rate value = 0.0001, Accuracy % = 46.15384615384615\n",
      "Learning rate value = 0.0002, Accuracy % = 61.53846153846154\n",
      "Learning rate value = 0.0003, Accuracy % = 64.1025641025641\n",
      "Learning rate value = 0.0005, Accuracy % = 71.7948717948718\n",
      "Learning rate value = 0.001, Accuracy % = 71.7948717948718\n",
      "Learning rate value = 0.05, Accuracy % = 74.35897435897436\n",
      "Learning rate value = 0.01, Accuracy % = 74.35897435897436\n",
      "Learning rate value = 0.1, Accuracy % = 74.35897435897436\n",
      "Learning rate value = 0.15, Accuracy % = 74.35897435897436\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.00005, 0.00006, 0.00007, 0.0001, 0.0002, 0.0003, 0.0005, 0.0010, 0.050, 0.0100, 0.100, 0.150] \n",
    "epochs = 100000\n",
    "\n",
    "for alpha in alphas:\n",
    "    theta1_x = gradient_descent(X1_train,Y_train,weights_1,alpha,0,epochs) \n",
    "    hyp = sigmoid(X1_test@theta1_x.T)\n",
    "    hyp = ((hyp>=0.5)*1).reshape(-1)\n",
    "    accuracy = (np.sum(hyp==Y_test)/Y_test.shape[0])*100\n",
    "    print(\"Learning rate value = \" + str(alpha) + \", Accuracy % = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda = 0\n",
      "Learning rate value: 5e-05, Accuracy % = 46.15384615384615\n",
      "Learning rate value: 6e-05, Accuracy % = 43.58974358974359\n",
      "Learning rate value: 7e-05, Accuracy % = 43.58974358974359\n",
      "Learning rate value: 0.0001, Accuracy % = 46.15384615384615\n",
      "Learning rate value: 0.0002, Accuracy % = 61.53846153846154\n",
      "Learning rate value: 0.0003, Accuracy % = 64.1025641025641\n",
      "Learning rate value: 0.0005, Accuracy % = 71.7948717948718\n",
      "Learning rate value: 0.001, Accuracy % = 71.7948717948718\n",
      "Learning rate value: 0.05, Accuracy % = 74.35897435897436\n",
      "Learning rate value: 0.01, Accuracy % = 74.35897435897436\n",
      "Learning rate value: 0.1, Accuracy % = 74.35897435897436\n",
      "Learning rate value: 0.15, Accuracy % = 74.35897435897436\n",
      "End of iteration \n",
      "\n",
      "Lambda = 0.001\n",
      "Learning rate value: 5e-05, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 6e-05, Accuracy % = 51.28205128205128\n",
      "Learning rate value: 7e-05, Accuracy % = 51.28205128205128\n",
      "Learning rate value: 0.0001, Accuracy % = 56.41025641025641\n",
      "Learning rate value: 0.0002, Accuracy % = 53.84615384615385\n",
      "Learning rate value: 0.0003, Accuracy % = 58.97435897435898\n",
      "Learning rate value: 0.0005, Accuracy % = 64.1025641025641\n",
      "Learning rate value: 0.001, Accuracy % = 64.1025641025641\n",
      "Learning rate value: 0.05, Accuracy % = 74.35897435897436\n",
      "Learning rate value: 0.01, Accuracy % = 74.35897435897436\n",
      "Learning rate value: 0.1, Accuracy % = 74.35897435897436\n",
      "Learning rate value: 0.15, Accuracy % = 74.35897435897436\n",
      "End of iteration \n",
      "\n",
      "Lambda = 0.005\n",
      "Learning rate value: 5e-05, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 6e-05, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 7e-05, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.0001, Accuracy % = 53.84615384615385\n",
      "Learning rate value: 0.0002, Accuracy % = 51.28205128205128\n",
      "Learning rate value: 0.0003, Accuracy % = 51.28205128205128\n",
      "Learning rate value: 0.0005, Accuracy % = 51.28205128205128\n",
      "Learning rate value: 0.001, Accuracy % = 53.84615384615385\n",
      "Learning rate value: 0.05, Accuracy % = 74.35897435897436\n",
      "Learning rate value: 0.01, Accuracy % = 66.66666666666666\n",
      "Learning rate value: 0.1, Accuracy % = 74.35897435897436\n",
      "Learning rate value: 0.15, Accuracy % = 74.35897435897436\n",
      "End of iteration \n",
      "\n",
      "Lambda = 0.01\n",
      "Learning rate value: 5e-05, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 6e-05, Accuracy % = 46.15384615384615\n",
      "Learning rate value: 7e-05, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.0001, Accuracy % = 51.28205128205128\n",
      "Learning rate value: 0.0002, Accuracy % = 51.28205128205128\n",
      "Learning rate value: 0.0003, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.0005, Accuracy % = 51.28205128205128\n",
      "Learning rate value: 0.001, Accuracy % = 51.28205128205128\n",
      "Learning rate value: 0.05, Accuracy % = 71.7948717948718\n",
      "Learning rate value: 0.01, Accuracy % = 66.66666666666666\n",
      "Learning rate value: 0.1, Accuracy % = 74.35897435897436\n",
      "Learning rate value: 0.15, Accuracy % = 74.35897435897436\n",
      "End of iteration \n",
      "\n",
      "Lambda = 0.05\n",
      "Learning rate value: 5e-05, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 6e-05, Accuracy % = 46.15384615384615\n",
      "Learning rate value: 7e-05, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.0001, Accuracy % = 51.28205128205128\n",
      "Learning rate value: 0.0002, Accuracy % = 51.28205128205128\n",
      "Learning rate value: 0.0003, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.0005, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.001, Accuracy % = 51.28205128205128\n",
      "Learning rate value: 0.05, Accuracy % = 66.66666666666666\n",
      "Learning rate value: 0.01, Accuracy % = 53.84615384615385\n",
      "Learning rate value: 0.1, Accuracy % = 66.66666666666666\n",
      "Learning rate value: 0.15, Accuracy % = 69.23076923076923\n",
      "End of iteration \n",
      "\n",
      "Lambda = 0.1\n",
      "Learning rate value: 5e-05, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 6e-05, Accuracy % = 46.15384615384615\n",
      "Learning rate value: 7e-05, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.0001, Accuracy % = 51.28205128205128\n",
      "Learning rate value: 0.0002, Accuracy % = 51.28205128205128\n",
      "Learning rate value: 0.0003, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.0005, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.001, Accuracy % = 51.28205128205128\n",
      "Learning rate value: 0.05, Accuracy % = 56.41025641025641\n",
      "Learning rate value: 0.01, Accuracy % = 51.28205128205128\n",
      "Learning rate value: 0.1, Accuracy % = 66.66666666666666\n",
      "Learning rate value: 0.15, Accuracy % = 66.66666666666666\n",
      "End of iteration \n",
      "\n",
      "Lambda = 0.5\n",
      "Learning rate value: 5e-05, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 6e-05, Accuracy % = 46.15384615384615\n",
      "Learning rate value: 7e-05, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.0001, Accuracy % = 51.28205128205128\n",
      "Learning rate value: 0.0002, Accuracy % = 51.28205128205128\n",
      "Learning rate value: 0.0003, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.0005, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.001, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.05, Accuracy % = 51.28205128205128\n",
      "Learning rate value: 0.01, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.1, Accuracy % = 53.84615384615385\n",
      "Learning rate value: 0.15, Accuracy % = 53.84615384615385\n",
      "End of iteration \n",
      "\n",
      "Lambda = 1\n",
      "Learning rate value: 5e-05, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 6e-05, Accuracy % = 46.15384615384615\n",
      "Learning rate value: 7e-05, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.0001, Accuracy % = 51.28205128205128\n",
      "Learning rate value: 0.0002, Accuracy % = 51.28205128205128\n",
      "Learning rate value: 0.0003, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.0005, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.001, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.05, Accuracy % = 51.28205128205128\n",
      "Learning rate value: 0.01, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.1, Accuracy % = 51.28205128205128\n",
      "Learning rate value: 0.15, Accuracy % = 53.84615384615385\n",
      "End of iteration \n",
      "\n",
      "Lambda = 5\n",
      "Learning rate value: 5e-05, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 6e-05, Accuracy % = 46.15384615384615\n",
      "Learning rate value: 7e-05, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.0001, Accuracy % = 51.28205128205128\n",
      "Learning rate value: 0.0002, Accuracy % = 51.28205128205128\n",
      "Learning rate value: 0.0003, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.0005, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.001, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.05, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.01, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.1, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.15, Accuracy % = 48.717948717948715\n",
      "End of iteration \n",
      "\n",
      "Lambda = 10\n",
      "Learning rate value: 5e-05, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 6e-05, Accuracy % = 46.15384615384615\n",
      "Learning rate value: 7e-05, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.0001, Accuracy % = 51.28205128205128\n",
      "Learning rate value: 0.0002, Accuracy % = 51.28205128205128\n",
      "Learning rate value: 0.0003, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.0005, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.001, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.05, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.01, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.1, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.15, Accuracy % = 48.717948717948715\n",
      "End of iteration \n",
      "\n",
      "Lambda = 20\n",
      "Learning rate value: 5e-05, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 6e-05, Accuracy % = 46.15384615384615\n",
      "Learning rate value: 7e-05, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.0001, Accuracy % = 51.28205128205128\n",
      "Learning rate value: 0.0002, Accuracy % = 51.28205128205128\n",
      "Learning rate value: 0.0003, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.0005, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.001, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.05, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.01, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.1, Accuracy % = 48.717948717948715\n",
      "Learning rate value: 0.15, Accuracy % = 48.717948717948715\n",
      "End of iteration \n",
      "\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.00005, 0.00006, 0.00007, 0.0001, 0.0002, 0.0003, 0.0005, 0.0010, 0.050, 0.0100, 0.100, 0.150] \n",
    "lambdas = [0,0.001,0.005,0.01,0.05,0.1,0.5,1,5,10,20]\n",
    "epochs = 100000\n",
    "\n",
    "for lambda1 in lambdas:\n",
    "    print(\"Lambda = \" + str(lambda1))\n",
    "    for alpha in alphas:\n",
    "        theta1_x = gradient_descent(X1_train,Y_train,weights_1,alpha,lambda1,epochs)\n",
    "        hypothesis = sigmoid(X1_test@theta1_x.T)\n",
    "        hypothesis = (hypothesis>=0.5)*1\n",
    "        hypothesis = hypothesis.reshape(-1)\n",
    "        acc = (np.sum(hypothesis==Y_test)/Y_test.shape[0])*100\n",
    "        print(\"Learning rate value: \" + str(alpha) + \", Accuracy % = \" + str(acc))\n",
    "    print(\"End of iteration \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONCLUSION\n",
    "\n",
    "Hence, we conclude that after changing the hypothesis, our accuracy is certainly much higher than what we were getting from a simple linear hypothesis (for particular values of regularisation parameter lambda and the learning rate). Since the boundary is not perfectly circular as well, we have to introduce some more advanced features into the hypothesis, and even perhaps some non linear ones. There is definitely scope for improvement, but we are getting an acceptable accuracy of around 75% with the assumption of a circular boundary for the hypothesis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
